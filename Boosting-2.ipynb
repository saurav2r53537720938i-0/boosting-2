{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfda7cf-84c4-4783-8058-17bcd7f995a4",
   "metadata": {},
   "source": [
    "Gradient boosting regression is a machine learning technique used for regression tasks.it works by sequentially adding predctors to an ensemble each one correcting the error made by the previous predictors it builds the model in a stage-wise fashion and optmize a loss function at each stage.Gradient boosting regression typically used decsion trees as the abse learners uses decsion optmizes the model performance by miniimizing the rsidual errors.it is known for its high predctive accuracy and robustness against overfittiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4894908-8f68-4bba-9372-285453309e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1707918185.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    def fit(self,x,y):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self,n_estimators=100,learning_rate=0.1,max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth\n",
    "        self.estimatnces = []\n",
    "         def fit(self,x,y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e7bc54-78ef-4218-9516-d78f1cbbbde9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 7) (2259874865.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    max_depth':[2,3,4]}\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_slection import\n",
    "gridsearchCV\n",
    "\n",
    "# Define the parmters\n",
    "param_grid = {n'estimators':[50,100,150]\n",
    "              learning_rate:[0.01,0.1,0.1,0.5],\n",
    "              max_depth':[2,3,4]}\n",
    "              \n",
    "# Create a gradient boosting regressor object\n",
    "              gb_regressor=\n",
    "              GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd6ddb-2a66-4d2e-8100-9e8ecba31eb1",
   "metadata": {},
   "source": [
    "In the context gradient boosting a weak learner refers to a base model that performs slightly better than random guessing on a given problem typically decsion tree with shadow gradient boosting these decsion tree with swallows depth are user as weak lerners in gradient boosting these descion tree are often are offered to as stumps when they have very few nodes.The idea behind using weak learners in gradient boosting is to iteratively improve the models performance by squentially adding weak learner to correct the error made by the previous one each weak learner focuses or learning reside between get value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89112d-f3b9-44b5-9d21-6cffdff28696",
   "metadata": {},
   "source": [
    "The gradient boosting algorthim was introduced by jereom frideman in 1999.He published a paper tired greedy Function approximatioon A gradient boosting machine along with trevor hastle and robert tibshiran while theyy do not reprsenta particular institution for this work they are all are reowned statisticaian and machine learning resarcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d9e30-159e-4c27-bb48-37ccf21efdae",
   "metadata": {},
   "source": [
    "Gradient boosting algorthim builds a weak learner typically a decsion tree with a swallow depth in a squential manner to minimize the loss function\n",
    "1. Intialize with a simple model\n",
    "2. Calculate residual\n",
    "3. Fit a weak learner to the residual\n",
    "4. update the model\n",
    "5. Rpeat\n",
    "6. Final ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
